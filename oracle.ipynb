{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Oracle: Predict your original project's success**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pyaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import IPython\n",
    "import struct\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Real Time Audio Capture & Waveform Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('TkAgg') # using tkinter for visualization <3\n",
    "\n",
    "CHUNK = 1024 * 4  # 4096 samples per chunk\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # mono sound because only one mic\n",
    "RATE = 44100 #standard rate 44.1 khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True, #initialize as true\n",
    "    output=True, #initialize as true\n",
    "    frames_per_buffer=CHUNK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.ion()  # interactive mode on\n",
    "x = np.arange(0, 2 * CHUNK)  # x values should match the length of data_int\n",
    "line, = ax.plot(x, np.random.rand(2 * CHUNK), color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 8192.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set y-axis limits to fit 16-bit audio\n",
    "ax.set_ylim(-2000, 2000)\n",
    "ax.set_xlim(0, 2 * CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Amplitude')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set plot labels and title\n",
    "ax.set_title(\"Real-Time Audio Waveform\")\n",
    "ax.set_xlabel(\"Sample Index\")\n",
    "ax.set_ylabel(\"Amplitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped by user.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        # read the audio data\n",
    "        data = stream.read(CHUNK * 2)\n",
    "\n",
    "        # unpack the data into 16-bit integers and center it around zero\n",
    "        data_int = np.array(struct.unpack(str(2 * CHUNK) + 'h', data), dtype='int16')\n",
    "\n",
    "        # update the line plot with new data\n",
    "        line.set_ydata(data_int)\n",
    "\n",
    "        # draw and flush the plot\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(0.01)  # Small pause to give GUI time to update\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped by user.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: User-Uploaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 44100\n",
      "Audio Data: [ 0.000000e+00  0.000000e+00  0.000000e+00 ... -9.304419e-14 -9.690668e-14\n",
      " -8.942789e-14]\n"
     ]
    }
   ],
   "source": [
    "# use librosa for audio files, whereas pyaudio is used for real-time audio streams\n",
    "test_audio = 'C:\\\\Users\\\\adith\\\\OneDrive\\\\Desktop\\\\vs code xoxo\\\\personal\\\\Oracle\\\\oracle_venv\\\\fresh-clone\\\\california_world.mp3'\n",
    "y, sr = librosa.load(test_audio)\n",
    "\n",
    "#load in the file: \n",
    "\n",
    "audio_data, sample_rate = librosa.load(test_audio, sr=None)\n",
    "\n",
    "#print to check: \n",
    "\n",
    "print(f'Sample Rate: {sample_rate}')\n",
    "print(f'Audio Data: {audio_data}')\n",
    "\n",
    "\n",
    "#plot waveform for funzies: \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, color='pink')\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for the Uploaded Waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs: \n",
    "- a representation of short-term power spectrum of a sound \n",
    "- based on a linear cosine transform of a log of the power spectrum on a non-linear mel scale\n",
    "- commonly used in speech & audio processing tasks \n",
    "- can recognize emotions in a speaker's voice and classify music into genres \n",
    "\n",
    "Chroma Features: \n",
    "- chroma features represent the 12 different pitch classes of a musical octave \n",
    "- really good at picking up slight changes in timbre & instrumentation \n",
    "- used in music retrieval tasks like chord recognition & key detection \n",
    "- a way to represent the tonal context of a musical audio signal in a condensed form \n",
    "\n",
    "Spectral Contrast: \n",
    "- measures the difference in amplitude between peaks and valleys in a sound spectrum \n",
    "- captures the relative distribution of energy across the frequency spectrum and is useful for distinguishing between different types of sounds such as speech and music\n",
    "\n",
    "Tonnetz: \n",
    "- represent the harmonic & melodic components of music \n",
    "- captures relationships between pitches and is also useful for key detection and chord recognition\n",
    "\n",
    "Mel-Scaled Spectrogram: \n",
    "- a spectrogram where the frequencies are scaled according the mel scale, which approximates the human ear's perception of sound \n",
    "- useful for visualizing the frequencies within an audio signal in a way that lines up with the way our ears would hear it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#step 1: Extract features:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m mfccs \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39m\u001b[43my\u001b[49m, sr\u001b[38;5;241m=\u001b[39msr, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[0;32m      4\u001b[0m chroma \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m      5\u001b[0m spectral_contrast \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "#step 1: Extract features:\n",
    "\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mfccs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#step 2: Combine features: \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m mfccs_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mmfccs\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m chroma_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(chroma, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m spectral_contrast_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(spectral_contrast, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mfccs' is not defined"
     ]
    }
   ],
   "source": [
    "#step 2: Combine features: \n",
    "\n",
    "mfccs_mean = np.mean(mfccs, axis=1)\n",
    "chroma_mean = np.mean(chroma, axis=1)\n",
    "spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
    "tonnetz_mean = np.mean(tonnetz, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mfccs_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#step 3: Combine into a single feature vector\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m feature_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([\u001b[43mmfccs_mean\u001b[49m, chroma_mean, spectral_contrast_mean, tonnetz_mean])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mfccs_mean' is not defined"
     ]
    }
   ],
   "source": [
    "#step 3: Combine into a single feature vector\n",
    "feature_vector = np.hstack([mfccs_mean, chroma_mean, spectral_contrast_mean, tonnetz_mean])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oracle_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
