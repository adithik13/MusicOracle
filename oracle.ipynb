{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Oracle: Predict your original project's success**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pyaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import IPython\n",
    "import struct\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Real Time Audio Capture & Waveform Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('TkAgg') # using tkinter for visualization <3\n",
    "\n",
    "CHUNK = 1024 * 4  # 4096 samples per chunk\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1  # mono sound because only one mic\n",
    "RATE = 44100 #standard rate 44.1 khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True, #initialize as true\n",
    "    output=True, #initialize as true\n",
    "    frames_per_buffer=CHUNK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.ion()  # interactive mode on\n",
    "x = np.arange(0, 2 * CHUNK)  # x values should match the length of data_int\n",
    "line, = ax.plot(x, np.random.rand(2 * CHUNK), color='pink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 8192.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set y-axis limits to fit 16-bit audio\n",
    "ax.set_ylim(-2000, 2000)\n",
    "ax.set_xlim(0, 2 * CHUNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(17.972222222222214, 0.5, 'Amplitude')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set plot labels and title\n",
    "ax.set_title(\"Real-Time Audio Waveform\")\n",
    "ax.set_xlabel(\"Sample Index\")\n",
    "ax.set_ylabel(\"Amplitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream stopped by user.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        # read the audio data\n",
    "        data = stream.read(CHUNK * 2)\n",
    "\n",
    "        # unpack the data into 16-bit integers and center it around zero\n",
    "        data_int = np.array(struct.unpack(str(2 * CHUNK) + 'h', data), dtype='int16')\n",
    "\n",
    "        # update the line plot with new data\n",
    "        line.set_ydata(data_int)\n",
    "\n",
    "        # draw and flush the plot\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(0.01)  # Small pause to give GUI time to update\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stream stopped by user.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: User-Uploaded Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Rate: 44100\n",
      "Audio Data: [ 0.000000e+00  0.000000e+00  0.000000e+00 ... -9.304419e-14 -9.690668e-14\n",
      " -8.942789e-14]\n"
     ]
    }
   ],
   "source": [
    "# use librosa for audio files, whereas pyaudio is used for real-time audio streams\n",
    "test_audio = 'C:\\\\Users\\\\adith\\\\OneDrive\\\\Desktop\\\\vs code xoxo\\\\personal\\\\Oracle\\\\oracle_venv\\\\fresh-clone\\\\california_world.mp3'\n",
    "y, sr = librosa.load(test_audio)\n",
    "\n",
    "#load in the file: \n",
    "\n",
    "audio_data, sample_rate = librosa.load(test_audio, sr=None)\n",
    "\n",
    "#print to check: \n",
    "\n",
    "print(f'Sample Rate: {sample_rate}')\n",
    "print(f'Audio Data: {audio_data}')\n",
    "\n",
    "\n",
    "#plot waveform for funzies: \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "librosa.display.waveshow(audio_data, sr=sample_rate, color='pink')\n",
    "plt.title('Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for the Uploaded Waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCCs: \n",
    "- a representation of short-term power spectrum of a sound \n",
    "- based on a linear cosine transform of a log of the power spectrum on a non-linear mel scale\n",
    "- commonly used in speech & audio processing tasks \n",
    "- can recognize emotions in a speaker's voice and classify music into genres \n",
    "\n",
    "Chroma Features: \n",
    "- chroma features represent the 12 different pitch classes of a musical octave \n",
    "- really good at picking up slight changes in timbre & instrumentation \n",
    "- used in music retrieval tasks like chord recognition & key detection \n",
    "- a way to represent the tonal context of a musical audio signal in a condensed form \n",
    "\n",
    "Spectral Contrast: \n",
    "- measures the difference in amplitude between peaks and valleys in a sound spectrum \n",
    "- captures the relative distribution of energy across the frequency spectrum and is useful for distinguishing between different types of sounds such as speech and music\n",
    "\n",
    "Tonnetz: \n",
    "- represent the harmonic & melodic components of music \n",
    "- captures relationships between pitches and is also useful for key detection and chord recognition\n",
    "\n",
    "Mel-Scaled Spectrogram: \n",
    "- a spectrogram where the frequencies are scaled according the mel scale, which approximates the human ear's perception of sound \n",
    "- useful for visualizing the frequencies within an audio signal in a way that lines up with the way our ears would hear it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# reload the audio file so following code blocks can execute the && jupyter notebooks doesn't get confused\n",
    "audio_path = 'C:\\\\Users\\\\adith\\\\OneDrive\\\\Desktop\\\\vs code xoxo\\\\personal\\\\Oracle\\\\oracle_venv\\\\fresh-clone\\\\california_world.mp3'\n",
    "y, sr = librosa.load(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: Extract features:\n",
    "\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: Combine features: \n",
    "\n",
    "mfccs_mean = np.mean(mfccs, axis=1)\n",
    "chroma_mean = np.mean(chroma, axis=1)\n",
    "spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
    "tonnetz_mean = np.mean(tonnetz, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: Combine into a single feature vector\n",
    "feature_vector = np.hstack([mfccs_mean, chroma_mean, spectral_contrast_mean, tonnetz_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.19439873e+02  1.15534729e+02  9.10197639e+00  1.58807144e+01\n",
      "  3.06296539e+00 -3.86641264e+00 -1.14549771e-01  8.58955669e+00\n",
      "  6.71586752e+00  7.82180738e+00  1.80883145e+00  1.00113688e+01\n",
      "  2.48616624e+00  2.90449053e-01  2.47736588e-01  3.50238234e-01\n",
      "  2.78483152e-01  2.96141565e-01  5.02131581e-01  3.39676291e-01\n",
      "  6.43634260e-01  3.14863056e-01  3.71400386e-01  5.44088900e-01\n",
      "  3.48741293e-01  1.98347736e+01  1.44098433e+01  2.00197972e+01\n",
      "  1.92636745e+01  1.83383434e+01  1.72727062e+01  4.70841222e+01\n",
      " -1.03588220e-01  1.89049989e-01  9.64397717e-02  1.87047864e-02\n",
      "  4.52142891e-02  5.08466687e-02]\n"
     ]
    }
   ],
   "source": [
    "#optional: print out the vector to see it for funzies\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model & Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/gtzan-genre\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/gtzan-genre loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "ds = deeplake.load(\"hub://activeloop/gtzan-genre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://activeloop/gtzan-genre', read_only=True, tensors=['audio', 'genre'])\n",
      "\n",
      " tensor      htype              shape             dtype  compression\n",
      " -------    -------            -------           -------  ------- \n",
      "  audio      audio     (1000, 660000:675808, 1)  float64    wav   \n",
      "  genre   class_label         (1000, 1)          uint32    None   \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ds.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. extract features from the audio dataset (GTZAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use sklearn <3 \n",
    "import numpy as np # need it to process the arrays \n",
    "import librosa # has the funcs we need like mel spectrogram etc \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "    return np.mean(mfccs.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some vars to hold the data\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:57<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm # i need a progress bar i have no patience\n",
    "\n",
    "for i in tqdm(range(len(ds))):\n",
    "    audio, genre = ds[i]['audio'].numpy().flatten(), ds[i]['genre'].numpy()[0]\n",
    "    features = extract_features(audio)\n",
    "    X.append(features)\n",
    "    y.append(genre)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oracle_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
